# -*- coding: utf-8 -*-
"""car price prediction with machine learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CzbF5u0DDhAsO-IiRB3nnz_PpQua60It
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

data =pd.read_csv('/content/car data.csv')

data.head()

data.tail()

data.describe()

# data cleaning
# check for missing values
print(data.isnull().sum())

# visualize data
# scatter plot for present as selling price
plt.figure(figsize=(10,6))
sns.scatterplot(x='Present_Price', y='Selling_Price', data=data)
plt.title('Present_Price vs Selling_Price')
plt.show()

# distribution of kms driven
plt.figure(figsize=(10,6))
sns.histplot(data['Driven_kms'], bins=30, kde=True)
plt.title('Distribution of Kms_Driven')
plt.xlabel('Driven_Kms')
plt.ylabel('Frequency')
plt.title('Present_Price vs Selling_Price')
plt.show()

# pairplot
sns.pairplot(data[['Year','Present_Price','Driven_kms','Owner','Selling_Price']])
plt.show()

# define the features and the target variable
features = ['Year','Present_Price','Driven_kms','Owner','Fuel_Type','Transmission']
target = 'Selling_Price'

# select the relevent columns from the original dataframes
x = data[features]
y = data[target]

# split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# define transformers fr numericl and categrial features
numeric_features = ['year', 'Present_Price', 'Driven_kms','Owner']
categorical_features = ['Fuel_Type', 'Transmission']

# Assuming x_train is your feature matrix and y_train is your target variable
# Assuming x_train has both numerical and categorical columns
# Identify categorical columns

categorical_columns = x_train.select_dtypes(include=['object']).columns

# Create a ColumnTransformer to handle numerical and categorical features separately
preprocessor = ColumnTransformer(
    transformers=[('num', StandardScaler(), x_train.columns.difference(categorical_columns)),
        ('cat', OneHotEncoder(), categorical_columns)])

# Create the pipeline with preprocessing and model fitting
pipeline = Pipeline([('preprocessor', preprocessor),('model', LinearRegression())])

# Fit the pipeline to your training data
pipeline.fit(x_train, y_train)

# can make predictions
y_predict = pipeline.predict(x_test)

print(pd.isnull(y_train).sum())
print(pd.isnull(y_predict).sum())

print(len(y_train), len(y_predict))

y_train = y_train.reset_index(drop=True)
y_predict = pd.Series(y_predict).reset_index(drop=True)

y_train = y_train.dropna()
y_predict = y_predict[~pd.isnull(y_train)]